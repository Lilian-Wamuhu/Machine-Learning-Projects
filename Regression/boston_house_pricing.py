# -*- coding: utf-8 -*-
"""Boston_House_Pricing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lTOVw6dTH1ph8gSR5hxtRfSxUjE2fZeg
"""

# Importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import datetime
import missingno as msno

# Importing data
test = pd.read_csv("test.txt")
train = pd.read_csv("train.txt")
train.head()

"""**Data Pre-Processing**"""

# Dimension of the dataset
train.shape

# Data distribution
train.describe()

train.info()

# Converting the column names to lower case
train.columns = map(str.lower,train.columns)
train.head()

# Checking the datatype 
train.dtypes

"""**Missing Values**"""

# Finding missing values
missing_data = train.isnull().sum().sort_values(ascending = False)
missing_data.head(20)

# Missing values percentage
total_missing=train.isnull().sum().sort_values(ascending=False)
percent=train.isnull().sum().sort_values(ascending=False)/train.isnull().count().sort_values(ascending=False)
missing_df=pd.concat([total_missing,percent],axis=1,keys=['total_missing','percentage_missing'],sort=False)
missing_df.index.name='variable'# set the index name to variable 
missing_df=missing_df[total_missing>0] # only retain those with missing values 
missing_df['percentage_missing']=missing_df['percentage_missing'].map(lambda x:x*100) # convert to percentage 
missing_df

"""**Categorical data**"""

#Checking the distribution of the categorical data

categorical=train.select_dtypes(include=[np.object])
categorical.head()

# handling missing values
categorical.fillna('None').head()

"""**Numerical data**"""

for var in ['lotfrontage','garageyrblt','masvnrarea']:
    train[var]=train[var].fillna(train[var].mode()[0])

train.isna().sum().sort_values(ascending=False)

"""**Feature colleration**"""

# Finding out the correlation between the features
corr = train.corr()
corr.shape

# Plotting price correlation
price = train.drop('saleprice', axis=1)
#creating a correlation matrix
correlations = price.corr()
sns.heatmap(correlations,square = True, cmap = "YlGnBu")
plt.yticks(rotation=0)
plt.xticks(rotation=90)
plt.show()


#price = train.drop('saleprice', axis=1)
#corr = price.corr()
#sns.heatmap(corr,cmap = "YlGnBu")

"""**Hot Encoding**"""

# Label Encoding for object to numeric conversion
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

CatVariables = train.select_dtypes(include=['object'])
for feature in CatVariables:
    train[feature] = le.fit_transform(train[feature].astype(str))

"""**Detecting Outliers**"""

from sklearn.ensemble import IsolationForest 
outliers = IsolationForest(contamination=0.05).fit_predict(train)
print('Rows with outliers: ', np.where(outliers))

train = train.drop(np.where(outliers<1)[0])
train.shape

"""**Data train and split**"""

# Spliting target variable and independent variables
X = train.drop(columns=['id','saleprice'])
y = train['saleprice']

print(len(X), len(y))

# Splitting to training and testing data
from sklearn.model_selection import train_test_split
#testing data size is of 33% of entire data
X_train, x_test, y_train, y_test =train_test_split(X,y, test_size = 0.33, random_state =5)

"""**1. Linear Regression**

**a. Model training**
"""

from sklearn.linear_model import LinearRegression
#fitting our model to train and test
lm = LinearRegression()
# Train the model using the training sets 
model = lm.fit(X_train,y_train)

"""**b. Model evaluation**"""

# Model prediction on train data
pred_y = lm.predict(x_test)
pd.DataFrame({"Actual": y_test, "Predict": pred_y}).head()

# Model Evaluation
import sklearn
from sklearn.metrics import r2_score
mse = sklearn.metrics.mean_squared_error(y_test, pred_y)
r2_score = mse = r2_score(y_test, pred_y)
print("mse is:", mse)
print("r2_score is:",r2_score )

"""𝑅^2 : It is a measure of the linear relationship between X and Y. It is interpreted as the proportion of the variance in the dependent variable that is predictable from the independent variable.

MSE: The mean square error (MSE) is just like the MAE, but squares the difference before summing them all instead of using the absolute value. 

"""

from sklearn.preprocessing import PolynomialFeatures, RobustScaler, StandardScaler
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import Lasso,Ridge
pipe = make_pipeline(PolynomialFeatures(degree=1),Ridge(alpha=100))
scores = cross_val_score(pipe, X, y, cv=3, scoring='neg_root_mean_squared_error')

print(scores, scores.mean())

# Visualizing the differences between actual prices and predicted values
plt.scatter(y_test,pred_y)
plt.xlabel('Saleprice')
plt.ylabel('Predicted Saleprice')
plt.title("Saleprices vs Predicted Saleprices")
plt.show()

"""**Test Data**"""

# Predicting Test data with the model
y_test_pred = lm.predict(x_test)

# Model Evaluation
import sklearn
from sklearn.metrics import r2_score
mse = sklearn.metrics.mean_squared_error(y_test, y_test_pred)
r2_score = mse = r2_score(y_test, y_test_pred)
print("mse is:", mse)
print("r2_score is:",r2_score )

"""Here the model evaluations scores are almost matching with that of train data. So the model is not overfitting.
Hence Linear Regression works best for this dataset

"""